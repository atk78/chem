# LSTMを用いた分子生成モデル(SMILES LSTM)

# 系列モデル
系列データとは、文字列のように1次元的に値が並んだデータを指す。系列長$T$の系列データを
$$
\bm{x} = (x_1, x_2, \dots, x_T) \in \mathcal{X}^{\top}
$$

のようにあらわすとする。ここで、$\mathcal{X}$は各値の取りうる範囲を表す集合である。また、ここでは離散的な集合とする。特に系列長を指定しない系列の集合は
$$
\mathcal{X}^* = \bigcup_{t=1}^{\infty} \mathcal{X}^t
$$
と表す。以下に系列データの例を示す

## 文字列
$\mathcal{X}$がアルファベットの集合$\{ a, b, \dots \}$のとき、$\bm{x} \in \mathcal{X}^*$は文字列を表現できる

## SMILES文字
$\mathcal{X}$がSMILESの記法で用いる文字集合$\{ C, O, N, = , \dots \}$であるとき、$\bm{x} \in \mathcal{X}^*$はSMILES系列を表現できる。

系列データを数式や機械学習のプログラム上で取り扱う際はデータが定義される値域$mathcal{X}$を非負整数集合
$$
\mathbb{Z}_{| \mathcal{X} |} = \{ 0, 1, \dots, |\mathcal{X} - 1| \}
$$
に変換したりワンホットベクトルの集合や埋め込み（embedding）に変換したものを用いることが多い。

## ワンホットベクトル
$$
x_d \in \{ 0, 1 \} かつ \sum_{d=1}^D x_d = 1
$$
を満たすとき、ワンホットベクトルという。D次元ワンホットベクトルの集合として、
$$
\bm{1}_d \in \mathbb{1}_D (d= 1, 2, \dots, D)
$$
つまり、d次元目が1となるワンホットベクトルとする。

## 埋め込み
集合$\mathcal{X}$の各要素$x \in \mathcal{X}$に対してパラメータをD個並べて定義される$D (\in \mathbb{N})$次元実数値ベクトル$\bm{x}_x \in \mathbb{R}^D$を$x$の埋め込みという。また、個の埋め込みの集合を$V(\mathcal{X})$と表す。

以上のように表現される系列データを取り扱うことを目的に作られたモデルを総称して**系列モデル**と呼ぶ。系列モデルには生成モデルと識別モデルという少なくとも2つのモデルがある。
- 生成モデル：系列$\bm{x} \in \mathcal{X}^*$の従う確率分布$p(\bm{x})$を直接モデル化するもの。
- 識別モデル：系列$\bm{x} \in \mathcal{X}^*$で条件付けたうえで、同じ長さの系列$\bm{y} \in \mathcal{Y}^*$の従う確率分布$p(\bm{y} | \bm{x})$をモデル化するもの。

## 系列モデルの学習
系列データの集合
$$
\mathcal{D} = \{(\bm{x}_n, \bm{y}_n) \in (\mathcal{X} \times \mathcal{Y})^T \}_{n=1}^N
$$
のようにパラメトリックな系列モデル$p_{\theta}(\bm{y} | \bm{x})$が与えられたもとで、$\mathcal{D}$を最もよく表現できるような系列モデルのパラメータ$\hat{\theta} \in \Theta$を学習する方法を考える。このような$\hat{\theta}$が得られれば、$p_{\theta*}$を用いて系列データ$\mathcal{D}$を再現できると期待される。つまり、入力$\bm{x}$が得られたときにモデル$p_{\hat{\theta}}$は$\bm{y}$を出力するが、この対$(\bm{x}, \bm{y})$は$\mathcal{D}$に含まれる系と近いものになると期待sれる。

パラメトリックな系列モデル$p_{\theta}(\bm{y} | \bm{x})$は予測分布の攻勢に従い、関数近似器、活性化関数、条件付確率分布を組み合わせたものとする。したがって、経験損失最小化として定式化でき、系列モデル$p_{\theta} (\bm{y} | \bm{x})$のパラメータ推定は、

$$
\textup{minimize}_{\theta \in \Theta} - \sum_{n=1}^N \sum_{t=1}^T \log p_{\theta}(y_{n, t} | x_{n, 1}, \dots, x_{n, t})
$$
のように経験損失を最小化する最適化問題として定式化される。この最適化問題を解くアルゴリズムも土曜のものを用いることができる。すなわち、自動微分を用いて目的関数の$\theta$に関する勾配を計算し、確率的勾配降下法を用いることで最適解$\hat{\theta} \in \Theta$を求めることができる。

### 系列識別モデルを系列生成モデルとして訓練する方法とそのアルゴリズム
- 入力：$p_{\theta}(x_t | x_1, \dots, x_{t-1}) (t=1, 2, \dots, T)$
- 出力：$X_1, X_2, \dots, X_T$の実現値

___
1: **for** $t = 1, 2, \dots, T$ **do**
2: &nbsp; &nbsp; &nbsp; &nbsp; $x_t \backsim p_{\theta}(\cdot | x_1, \dots, x_{t-1})$をサンプリング
3: **return** ${x_t}^T_{t=1}$
___

系列生成モデル用のデータセットを
$$
\begin{aligned}
\mathcal{D} &= \{ \bm{x}_n \}_{n=1}^N \\
\bm{x}_n &= (x_{n, 1}, x_{n, 2}, \dots, x_{n, T}) \in \mathcal{X}^T
\end{aligned}
$$

としたときに各系列$\bm{x}_n$について、次のようにデータを変換した後、系列モデルを訓練することで実現できる。

すなわち、アルファベット集合を
$$
\tilde{\mathcal{X}} := \mathcal{X} \cup \{\langle \textup{sos} \rangle,  \langle \textup{eos} \rangle\}
$$

と拡張して、
$$
\tilde{\bm{x}}_n = (\langle \textup{sos} \rangle, x_{n, 1}, \dots, x_{n, T-1}, x_{n, T}) \in \tilde{\mathcal{X}}^{T+1} \\
\tilde{\bm{y}}_n = (x_{n, 1}, \dots, x_{n, T-1}, x_{n, T}, \langle \textup{eos} \rangle) \in \tilde{\mathcal{X}}^{T+1}
$$
と変換する。そして、変換されたデータセット
$$
\tilde{\mathcal{D}} = \{ (\tilde{\bm{x}}_n, \tilde{\bm{y}}_n) \}_{n=1}^N
$$
を用いて最適化問題を解けば系列識別モデルを生成モデルとして学習することができる。

## 系列モデルの構成
活性化関数と条件付確率分布は個々の問題設定に応じて決まる。ここでは分子生成モデルを作ることを目的としているため、取り扱う系列データは多値ラベルの系列である。つまり、出力$y_{n, t} \in \mathcal{Y}$は$| \mathcal{Y} | $通りの値をとる。

一方、関数近似器は$x1, \dots, x_t$を入力とし、$\mathbb{R}^{|\mathcal{Y}|}$に属する実数値ベクトルを出力するように設計する必要がある。よって、関数近似器としては
$$
\hat{\bm{y}}_t = g_{\theta}(x_1, \dots, x_t) \in \mathcal{R}^{|\mathcal{Y}|}
$$
となる関数を用いればいい。このような系列モデルの関数近似器に使えるものとして**再帰型ニューラルネットワーク（RNN）**や**長・短期記憶（LSTM）**が知られている。

## 再帰型ニューラルネットワーク（RNN）
系列データを取り扱う関数近似器で満たすべき要件をまとめると以下のようになる<br>
- 1. **入出力の依存関係**をもつ
- 2. **系列長に依存しない**モデル

これらを両立することは簡単ではないが、これを解決する手法の一つがRNNである。

RNNでは時刻1から時刻tまでの情報を**隠れ状態**（hidden state）と呼ばれる固定長のベクトル$\bm{h}_t$で表現することが特徴である。隠れ状態を用いて出力を決めることで要件1を満たす。また、隠れ条件の時間発展のモデルを時刻によらないものとすることで要件2を満たすことができる。

RNNは複数の隠れ層を持ち得るが、まず入力に最も近い層を見てみる。

![RNNの入力層と1層目の変数の依存関係](images/240122162730.png)

1層目の隠れ状態$\bm{h}_t^{(1)} \in \mathbb{R}^{D^{(1)}}$に適当に初期値$\bm{h}_0^{{(1)}}$を与えたとき（$\bm{h}_0^{{(1)}} = 0$が多い）、次式で定義される。

$$
\bm{h}_t^{{(1)}} = \sigma (W_{ih}^{(1)} \bm{x}_t + W_{hh}^{(1)} \bm{h}_{t-1}^{(1)} + \bm{b}^{(1)}), (=1, 2, \dots, T)
$$

ここで、
$$
W_{ih}^{(1)} \in \mathbb{R}^{D^{(1)} \times D^{(0)}}, W_{hh}^{(1)} \in \mathbb{R}^{D^{(1)} \times D^{(1)}}, \bm{b}^{(1)} \in \mathbb{R}^{D^{(1)}}
$$

がそれぞれのモデルのパラメータである。添え字ih→入力から隠れ層、hh→隠れ層から隠れ層の結合に紐づく重みであることを示している。

上式によると時刻tの隠れ状態$\bm{h}_t^{(1)}$は、同時刻の入力$\bm{x}_t$と前時刻の隠れ状態$\bm{h}_{t-1}^{(1)}$によって決まる。つまり、RNNを用いると「現時刻の隠れ状態」が「前時刻の隠れ状態に依存する」という構造で、再帰的に依存関係が決まる。

このように隠れ状態を導入することで2つの要件を満たすニューラルネットワークを構築することができる。

### 多層化
上記のように層を単体で用いることもあるが、より複雑なモデルではこの層を複数重ねたうえで、別途出力のための層を重ねた関数近似器を用いる。ここで$l$層目（$l = 1, 2, \dots, L$）の時刻$t$の隠れ状態を$\bm{h}_t^{(l)} \in \mathbb{R}^{D^{(l)}}$とする。$\bm{h}_0^{(l)} \in \mathbb{R}^{D^{(l)}}$を適当に初期化したうえで$l$層目の隠れ状態の時間発展は

$$
\bm{h}_t^{(l)} = \sigma(W_{ih}^{l} \bm{h}_t^{l-1} + W_{hh}^{(l)} \bm{h}_{t-1}^{(l)} + \bm{b}^{(l)})
$$
で定義される。また各時刻$t=1, 2, \dots, T$での関数近似器の出力$\bar{\bm{y}}_t \in \mathbb{R}^{|\mathcal{Y}|}$は、
$$
\bar{\bm{y}}_t = W_{ho}\bm{h}_t^{(L)} + \bm{b}_o
$$
と計算する。ここで、$W_{ho} \in \mathbb{R}^{|\mathcal{Y}| \times D^{(L)}}$,$\bm{b}_o \in \mathbb{R}^{|\mathcal{Y}|}$を出力層のパラメータとする。

### 双方向化
これまでのRNNの説明では時刻の順に従って出力を構成するため、$\bar{\bm{y}}_t$は$\bm{x}_1, \dots, \bm{x}_t$には依存するが、$\bm{x}_{t+1}, \dots, \bm{x}_T$には依存しないモデルになっている。（時刻t=1, 2, 3, 4のとき、t=2はt=1と依存性があるが、t=3や4と依存性がない）そこで、これらを解決するための方法の1つが**双方向再帰型ニューラルネットワーク**（bidirectional RNN）である。

双方向RNNのアイデアは単純で、あるRNNとそれとは時間方向が逆のRNNを組み合わせれば入力系列全体を使って出力を決めることができるというものである。

### 勾配消失・爆発問題
RNNでは原理上すべての時刻における入力を考慮するが、時間的に遠い過去のデータほど寄与度の調整が難しくなる。例えば、RNNの時間発展で活性化関数がないとすると$\bm{h}_t^{(l)}$に対する$\bm{h}_1^{l}$の寄与はt-1乗となり、固有値の絶対値がすべて1より小さい場合指数的に小さくなる。逆に1より大きい固有値がある場合、指数的に大きくなってしまい、いずれの場合も寄与度の調整が難しい、つまり学習が難しい。
この問題を解決するには単純なRNNでは不十分で、ネットワークの構造を工夫する必要がある。その一例として長・短期記憶がある。

## 長・短期記憶（LSTM）
LSTMはRNNの勾配消失・爆発問題を解決するために考案されたニューラルネットワークである。LSTMユニットは内部状態として**細胞状態**（cell state）$\bm{c}_t$を持ち、隠れ状態$\bm{h}_t$を同時刻の細胞状態$\bm{c}_t$によって定義することが特徴である。細胞状態$\bm{c}_t$の時間発展は全時刻の細胞状態$\bm{c}_{t-1}$をそのまま維持するか、あるいは新しい状態$\bm{g}_t$に書き換えるかという選択的なものである。もし、全時刻の細胞状態をそのまま維持するという選択を続けた場合、長期間にわたって同じ細胞状態を維持、つまり、同じ隠れ状態$\bm{h}_t$を維持できるため、勾配消失・爆発問題を回避することができる。

LSTMは以下の3つの内部状態を持つ
- $\bm{h}_t$：時刻$t$の隠れ状態でLSTM細胞の出力及び次の時刻への入力に使われる
- $\bm{c}_t$：時刻$t$の細胞状態で、LSTM細胞の記憶に相当する
- $\bm{g}_t$：時刻$t$の新規記憶候補を表し、$\bm{c}_t$を$\bm{g}_t$に置き換えるかどうかの判断が毎時刻で行われる。

また、次の3つのゲートと呼ばれるタイプの変数が上記3つの内部状態それぞれの時間発展を司っている。
- $\bm{i}_t$：細胞状態$\bm{c}_t$を新規記憶候補$\bm{g}_t$で書き換える度合いを調整する入力ゲート
- $\bm{f}_t$：前の時刻の細胞状態$\bm{c}_{t-1}$を維持する度合いを調整する忘却ゲート
- $\bm{o}_t$：細胞状態を隠れ状態に反映する度合いを調整する出力ゲート

これらを数式で表現すると次のようになる
$$
\begin{aligned}
\bm{i}_t &= \sigma(W_{xi}\bm{x}_t + W_{hi} \bm{h}_{t-1} + \bm{b}_i) \\
\bm{f}_t &= \sigma(W_{xf}\bm{x}_t + W_{hf} \bm{h}_{t-1} + \bm{b}_f) \\
\bm{o}_t &= \sigma(W_{xo}\bm{x}_t + W_{ho} \bm{h}_{t-1} + \bm{b}_o) \\
\bm{g}_t &= \tanh(W_{xc}\bm{x}_t + W_{hc}\bm{h}_{t-1} + \bm{b}_c) \\
\bm{c}_t &= \bm{f}_t \odot \bm{c}_{t-1} + \bm{i}_t \odot \bm{g}_t \\
\bm{h}_t &= \bm{o}_t \odot \tanh(\bm{c}_t)
\end{aligned}
$$

$\odot$：アダマール積（Hadamard product）であり、ベクトルどうしの要素ごとの席を計算する演算子。

入力ゲート（$\bm{i}_t$）、忘却ゲート（$\bm{f}_t$）、出力ゲート（$\bm{o}_t$）はそれぞれ入力$\bm{x}_t$と、前時刻の隠れ状態$\bm{h}_{t-1}$の非線形関数としてモデル化され、また新規記憶候補$\bm{g}_t$も同様にモデル化される。

LSTMを関数近似器として用いるとき、その最終的な出力は隠れ状態を用いて次のように計算される。

$$
\bar{\bm{y}} = W_{hy} \bm{h}_t + \bm{b}_y
$$

# 系列モデル(LSTM)を用いた分子生成モデル
もっとも簡単な分子生成モデルとしてLSTMを用いて分子生成モデルを考える。ここではこのモデルをSMILES-LSTMと呼ぶこととする。

## データの取得と前処理
初めに分子のデータセットを取得し、系列モデルで取り扱うための前処理を行う。分子構造をSMILESで表すことでアルファベット集合$\mathcal{X}$を構築し、各SMILES系列$\bm{x} \in \mathcal{X}^*$から系列識別モデル用のデータ$(\tilde{\bm{x}}, \tilde{\bm{y}}) \in \tilde{\mathcal{X}^*} \times \tilde{\mathcal{X}^*}$をつくる。

ここではGuacamolと呼ばれる分子生成手法のベンチマークで使われる分子データセットを用いる。このデータセットではそれぞれの分子はSMILESで表現されている。このデータセットに対して次のように系列識別モデル用に変換する前処理を行う。

- 前処理1: 系列識別モデルで取り扱うために、データセットのSMILES系列それぞれに開始記号$\langle \mathrm{sos} \rangle$と終了記号$\langle \mathrm{eos} \rangle$を付け加える。
- 前処理2: すべての系列の長さをそろえるために、終了記号の後を空文字$\langle \mathrm{pad} \rangle$で埋める（**pading**）
- 前処理3: 全SMILES系列でつかわれる文字$\bar{\mathcal{X}} = \mathcal{X} \cup \{ \langle \mathrm{sos} \rangle, \langle \mathrm{eos} \rangle, \langle \mathrm{pad} \rangle \}$を集計し、$\bar{\mathcal{X}}$と整数の対応表を作る
