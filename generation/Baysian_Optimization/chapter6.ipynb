{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 分子生成モデルを用いた分子最適化\n",
    "\n",
    "## 6.1 分子最適化問題とその難しさ\n",
    "\n",
    "ここでは分子最適化問題を取り扱う。すなわち、評価関数を$f~*: \\mathcal{M} \\rightarrow \\mathbb{R}$として、\n",
    "$$\n",
    "m^* = \\argmax_{m \\in \\mathcal{M}} f^* (m)\n",
    "$$\n",
    "となる分子$m^* \\in \\mathcal{M}$を求める分子最適化問題を取り扱う。例えば、ある分聖地が$\\theta \\in \\mathbb{R}$という値をとる分子を求めたい場合にはその物性値を出力する関数$y: \\mathcal{M} \\rightarrow \\mathbb{R}$を用いて、\n",
    "$$\n",
    "f^*(m) = - \\frac{1}{2}(y(m) - \\theta)^2\n",
    "$$\n",
    "のような評価関数とすればよい。\n",
    "\n",
    "分子最適化問題の主な難しさとして、以下の二点の課題とその解決策が考えられる。\n",
    "\n",
    "### 1. 分子の空間が離散的である\n",
    "評価関数の定義息が分子からなる離散的な空間であることがあげられる。定義息が連続的であれば評価関数の勾配を用いることで解の改善をすることができる。しかし、定義息が離散的だと勾配情報のような解の改善に有用な情報が得られないため連続最適化よりも取り組むことが難しい。\n",
    "この課題に対する解決策として、分子の空間を連続的な潜在空間に変換するオートエンコーダを用いて、連続最適化問題に変換する方法と、強化学習を用いて離散的な空間で最適化問題を解く方法が知られている。6章では前者を、7章では後者を説明する。\n",
    "\n",
    "### 2. 評価関数に関する情報が限られている\n",
    "評価関数$f^*$に関して得られる情報は個々の問題設定によって異なる。もっとも情報が限られた問題設定では、有限個の分子に対する評価関数値のみが与えられた状況で分子最適化を行う場合があげられる。このような問題設定では**オフライン強化学習**が知られている。\n",
    "\n",
    "例：既存の実験データを用いて新規物質を発見したい\n",
    "\n",
    "もう1つの問題設定では、任意の分子$m \\in \\mathcal{M}$に対して、その評価関数の値$f^*(m)$は知ることができるが、それ以外の情報が得られていない状況で分子最適化を行う場合。一見して多くの情報が得られるように見えるが、標準的な最適化手法では評価関数の値だけでなく勾配の情報も必要になるため使用できる最適化手法が限定される。また、一般的に評価関数の値を得るコストは小さくないため評価関数の評価回数（試験回数）をなるべく少なくしたいという要請もある。このような問題設定では**ブラックボックス最適化**が知られており、その中で代表的なものとして**ベイズ最適化**が知られている。\n",
    "\n",
    "例：シミュレータを使って物性値を計算する場合や新たに実験を行って物性値を測定する場合\n",
    "\n",
    "### 2つの課題への対処方法\n",
    "- 分子の空間が離散的であること\n",
    "- 評価関数に関する情報が限られていること\n",
    "\n",
    "が課題となる。それらの課題について解決策を述べていく\n",
    "\n",
    "\n",
    "## 6.2 分子最適化問題の連続最適化問題への変換\n",
    "変分オートエンコーダを用いて離散的な分子グラフと実数値ベクトルを行き来することによって実現する。\n",
    "\n",
    "### 6.2.1 準備\n",
    "分子のデータセットを用いて学習した変分オートエンコーダを$q(\\bm{z} | m), p_{M|\\bm{Z}}(m | \\bm{z})$とする。ここで$m \\in \\mathcal{M}$は分子、$\\bm{n} \\in \\mathbb{R}^H$はそれに対応する潜在ベクトル、$q$はエンコーダ、$p_{M | \\bm{Z}}$はデコーダである。また、分子$m$を入力すると、その分子に対する評価関数の値$f^*(m)$を得ることができる。\n",
    "\n",
    "### 6.2.2 連続最適化問題への帰着\n",
    "まず、デコーダ$p_{M|\\bm{Z}}$と、評価関数$p_{Y|M}$を合成することで洗剤表現$\\bm{z} \\in \\mathbb{R}^H$で条件づけた下での、それに対応する分子の評価関数の値に対応する**確率変数**が得られる。つまり、$\\bm{z}$を入力すると$Y$が出力されるという確率的な入出力関係を表している。\n",
    "この入手鬱力関係を関数$f_Z: \\mathbb{R}^H \\rightarrow \\mathbb{R}$とノイズに相当する確率変数$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$を用いて\n",
    "$$\n",
    "y = f_z(\\bm{z}) + \\varepsilon\n",
    "$$\n",
    "とモデル化することを考える。この関数$f_Z$を最大化することで元の分子の最適化問題を近似的に解くことができる。最大化により得られた$\\bm{z}$にデコーダを用いることで分子に変換できる。\n",
    "\n",
    "### 6.2.3 $f_Z$の推定\n",
    "\n",
    "ここでは分子とその評価関数の値の対から成る既存のデータセット\n",
    "$$\n",
    "\\mathcal{D} = \\{(m_n, y_n) \\in \\mathcal{M} \\times \\mathbb{R} \\}_{n=1}^N\n",
    "$$\n",
    "を用いて目的関数$f_Z$を推定する方法を説明する。\n",
    "\n",
    "$f_Z$を推定するためには入力$\\bm{z}$と出力$f_Z{\\bm{z}}$の対からなる$\\mathcal{D}_Z$があればよい。適切な機械学習の手法を用いることで$\\mathcal{D}_Z$から$f_Z$を推定できるためである。このようなデータセット$\\mathcal{D}_Z$はエンコーダを使って作ることができる。\n",
    "\n",
    "## 6.3 ベイズ最適化を用いて分子最適化\n",
    "ここでは任意の入力$bm{z}$に対して目的関数$f_Z$の値を得ることができる場合の連続最適化問題を解く方法について説明する。このような最適化問題を**ブラックボックス最適化**と呼ぶ。様々な方法があるが今回はベイズ最適化を取り上げる。\n",
    "\n",
    "### 6.3.1 問題設定\n",
    "目的関数$f_Z: \\mathbb{R}^H \\rightarrow \\mathbb{R}$は入力が与えられた下で対応する出力の値を計算することができるがそれ以外の情報（勾配など）は得られないとする。このような関数をブラックボックス関数と呼ぶ。\n",
    "\n",
    "### 6.3.2 アルゴリズムの概要\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Crippen\n",
    "import torch\n",
    "# from torchdrug.data.molecule import PackedMolecule\n",
    "# from torchdrug.metrics import penalized_logP\n",
    "\n",
    "\n",
    "def filter_valid(smiles_list):\n",
    "    \"\"\"SMILES系列のリストを受け取り、正しく分子に変換できるものを抽出\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles_list : list[str]\n",
    "        _description_\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    success_list = []\n",
    "    fail_idx_list = []\n",
    "    for each_idx, each_smiles in enumerate(smiles_list):\n",
    "        try:\n",
    "            smiles = Chem.MolToSmiles(\n",
    "                Chem.MolFromSmiles(each_smiles))\n",
    "            success_list.append(smiles)\n",
    "        except:\n",
    "            fail_idx_list.append(each_idx)\n",
    "    return success_list, fail_idx_list\n",
    "\n",
    "\n",
    "def compute_plogp(smiles_list):\n",
    "    filtered_smiles_list, fail_idx_list = filter_valid(smiles_list)\n",
    "    if not filtered_smiles_list:\n",
    "        return -30.0 * torch.ones(len(smiles_list))\n",
    "    # packed_dataset = PackedMolecule.from_smiles(\n",
    "    #     filtered_smiles_list)\n",
    "    # _plogp_tensor = penalized_logP(packed_dataset)\n",
    "    packed_dataset = [Chem.MolFromSmiles(smi) for smi in smiles_list]\n",
    "    _plogp_tensor = torch.tensor([Crippen.MolLogP(mol) for mol in packed_dataset])\n",
    "    plogp_tensor = torch.zeros(len(smiles_list),\n",
    "                               dtype=torch.float)\n",
    "    each_other_idx = 0\n",
    "    for each_idx in range(len(plogp_tensor)):\n",
    "        if each_idx in fail_idx_list:\n",
    "            plogp_tensor[each_idx] = -30.0\n",
    "        else:\n",
    "            plogp_tensor[each_idx] = _plogp_tensor[each_other_idx]\n",
    "            each_other_idx += 1\n",
    "    return plogp_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programming\\python\\book\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 127310/127310 [00:02<00:00, 58105.67it/s]\n",
      "100%|██████████| 7956/7956 [00:00<00:00, 76789.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * 0\ttensor([2.8966], dtype=torch.float64)\n",
      " * 1\ttensor([0.6716], dtype=torch.float64)\n",
      " * 2\ttensor([5.0504], dtype=torch.float64)\n",
      " * 3\ttensor([1.7106], dtype=torch.float64)\n",
      " * 4\ttensor([0.4904], dtype=torch.float64)\n",
      " * 5\ttensor([-2.1848], dtype=torch.float64)\n",
      " * 6\ttensor([-0.9497], dtype=torch.float64)\n",
      " * 7\ttensor([2.8980], dtype=torch.float64)\n",
      " * 8\ttensor([4.1570], dtype=torch.float64)\n",
      " * 9\ttensor([7.7818], dtype=torch.float64)\n",
      " * 10\ttensor([0.1610], dtype=torch.float64)\n",
      " * 11\ttensor([3.3149], dtype=torch.float64)\n",
      " * 12\ttensor([5.2022], dtype=torch.float64)\n",
      " * 13\ttensor([4.6439], dtype=torch.float64)\n",
      " * 14\ttensor([2.2004], dtype=torch.float64)\n",
      " * 15\ttensor([-0.1827], dtype=torch.float64)\n",
      " * 16\ttensor([1.3911], dtype=torch.float64)\n",
      " * 17\ttensor([7.0427], dtype=torch.float64)\n",
      " * 18\ttensor([4.7053], dtype=torch.float64)\n",
      " * 19\ttensor([2.9968], dtype=torch.float64)\n",
      " * 20\ttensor([3.3650], dtype=torch.float64)\n",
      " * 21\ttensor([1.4717], dtype=torch.float64)\n",
      " * 22\ttensor([3.2338], dtype=torch.float64)\n",
      " * 23\ttensor([2.3550], dtype=torch.float64)\n",
      " * 24\ttensor([3.9953], dtype=torch.float64)\n",
      " * 25\ttensor([0.5834], dtype=torch.float64)\n",
      " * 26\ttensor([3.7695], dtype=torch.float64)\n",
      " * 27\ttensor([5.5768], dtype=torch.float64)\n",
      " * 28\ttensor([3.4507], dtype=torch.float64)\n",
      " * 29\ttensor([1.9196], dtype=torch.float64)\n",
      " * 30\ttensor([3.6487], dtype=torch.float64)\n",
      " * 31\ttensor([3.8618], dtype=torch.float64)\n",
      " * 32\ttensor([8.7945], dtype=torch.float64)\n",
      " * 33\ttensor([4.0319], dtype=torch.float64)\n",
      " * 34\ttensor([4.3049], dtype=torch.float64)\n",
      " * 35\ttensor([2.8812], dtype=torch.float64)\n",
      " * 36\ttensor([1.4830], dtype=torch.float64)\n",
      " * 37\ttensor([2.2093], dtype=torch.float64)\n",
      " * 38\ttensor([2.3842], dtype=torch.float64)\n",
      " * 39\ttensor([1.0626], dtype=torch.float64)\n",
      " * 40\ttensor([1.4735], dtype=torch.float64)\n",
      " * 41\ttensor([3.8320], dtype=torch.float64)\n",
      " * 42\ttensor([1.6260], dtype=torch.float64)\n",
      " * 43\ttensor([0.0968], dtype=torch.float64)\n",
      " * 44\ttensor([2.6822], dtype=torch.float64)\n",
      " * 45\ttensor([1.1402], dtype=torch.float64)\n",
      " * 46\ttensor([3.5376], dtype=torch.float64)\n",
      " * 47\ttensor([-1.5965], dtype=torch.float64)\n",
      " * 48\ttensor([0.8029], dtype=torch.float64)\n",
      " * 49\ttensor([5.0135], dtype=torch.float64)\n",
      " * 50\ttensor([0.7181], dtype=torch.float64)\n",
      " * 51\ttensor([3.7632], dtype=torch.float64)\n",
      " * 52\ttensor([3.8334], dtype=torch.float64)\n",
      " * 53\ttensor([2.1569], dtype=torch.float64)\n",
      " * 54\ttensor([5.0294], dtype=torch.float64)\n",
      " * 55\ttensor([3.4377], dtype=torch.float64)\n",
      " * 56\ttensor([4.0162], dtype=torch.float64)\n",
      " * 57\ttensor([4.6765], dtype=torch.float64)\n",
      " * 58\ttensor([2.1663], dtype=torch.float64)\n",
      " * 59\ttensor([-1.1692], dtype=torch.float64)\n",
      " * 60\ttensor([13.5115], dtype=torch.float64)\n",
      " * 61\ttensor([2.0553], dtype=torch.float64)\n",
      " * 62\ttensor([5.3772], dtype=torch.float64)\n",
      " * 63\ttensor([1.1334], dtype=torch.float64)\n",
      " * 64\ttensor([3.1288], dtype=torch.float64)\n",
      " * 65\ttensor([5.9967], dtype=torch.float64)\n",
      " * 66\ttensor([5.0593], dtype=torch.float64)\n",
      " * 67\ttensor([0.6175], dtype=torch.float64)\n",
      " * 68\ttensor([3.0074], dtype=torch.float64)\n",
      " * 69\ttensor([5.8944], dtype=torch.float64)\n",
      " * 70\ttensor([4.2353], dtype=torch.float64)\n",
      " * 71\ttensor([2.5991], dtype=torch.float64)\n",
      " * 72\ttensor([3.4312], dtype=torch.float64)\n",
      " * 73\ttensor([3.6506], dtype=torch.float64)\n",
      " * 74\ttensor([1.4797], dtype=torch.float64)\n",
      " * 75\ttensor([1.6861], dtype=torch.float64)\n",
      " * 76\ttensor([4.2454], dtype=torch.float64)\n",
      " * 77\ttensor([1.5270], dtype=torch.float64)\n",
      " * 78\ttensor([2.6767], dtype=torch.float64)\n",
      " * 79\ttensor([2.2080], dtype=torch.float64)\n",
      " * 80\ttensor([2.7664], dtype=torch.float64)\n",
      " * 81\ttensor([1.5620], dtype=torch.float64)\n",
      " * 82\ttensor([1.5129], dtype=torch.float64)\n",
      " * 83\ttensor([-1.1962], dtype=torch.float64)\n",
      " * 84\ttensor([2.8121], dtype=torch.float64)\n",
      " * 85\ttensor([2.5302], dtype=torch.float64)\n",
      " * 86\ttensor([3.8116], dtype=torch.float64)\n",
      " * 87\ttensor([2.0425], dtype=torch.float64)\n",
      " * 88\ttensor([9.2520], dtype=torch.float64)\n",
      " * 89\ttensor([1.7731], dtype=torch.float64)\n",
      " * 90\ttensor([0.8780], dtype=torch.float64)\n",
      " * 91\ttensor([1.1919], dtype=torch.float64)\n",
      " * 92\ttensor([-2.3161], dtype=torch.float64)\n",
      " * 93\ttensor([5.0190], dtype=torch.float64)\n",
      " * 94\ttensor([5.0652], dtype=torch.float64)\n",
      " * 95\ttensor([0.9478], dtype=torch.float64)\n",
      " * 96\ttensor([2.3673], dtype=torch.float64)\n",
      " * 97\ttensor([0.9241], dtype=torch.float64)\n",
      " * 98\ttensor([3.0166], dtype=torch.float64)\n",
      " * 99\ttensor([5.0884], dtype=torch.float64)\n",
      " * 100\ttensor([7.2113], dtype=torch.float64)\n",
      " * 101\ttensor([2.2104], dtype=torch.float64)\n",
      " * 102\ttensor([0.4455], dtype=torch.float64)\n",
      " * 103\ttensor([3.0340], dtype=torch.float64)\n",
      " * 104\ttensor([3.0072], dtype=torch.float64)\n",
      " * 105\ttensor([-0.0878], dtype=torch.float64)\n",
      " * 106\ttensor([1.8463], dtype=torch.float64)\n",
      " * 107\ttensor([2.0851], dtype=torch.float64)\n",
      " * 108\ttensor([2.3190], dtype=torch.float64)\n",
      " * 109\ttensor([0.7402], dtype=torch.float64)\n",
      " * 110\ttensor([4.4049], dtype=torch.float64)\n",
      " * 111\ttensor([9.3258], dtype=torch.float64)\n",
      " * 112\ttensor([1.8401], dtype=torch.float64)\n",
      " * 113\ttensor([8.0505], dtype=torch.float64)\n",
      " * 114\ttensor([2.5578], dtype=torch.float64)\n",
      " * 115\ttensor([0.8197], dtype=torch.float64)\n",
      " * 116\ttensor([1.1115], dtype=torch.float64)\n",
      " * 117\ttensor([0.6327], dtype=torch.float64)\n",
      " * 118\ttensor([0.1330], dtype=torch.float64)\n",
      " * 119\ttensor([3.2183], dtype=torch.float64)\n",
      " * 120\ttensor([-0.7243], dtype=torch.float64)\n",
      " * 121\ttensor([4.0445], dtype=torch.float64)\n",
      " * 122\ttensor([0.4061], dtype=torch.float64)\n",
      " * 123\ttensor([-2.3978], dtype=torch.float64)\n",
      " * 124\ttensor([1.1515], dtype=torch.float64)\n",
      " * 125\ttensor([1.9401], dtype=torch.float64)\n",
      " * 126\ttensor([2.5112], dtype=torch.float64)\n",
      " * 127\ttensor([-30.], dtype=torch.float64)\n",
      " * 128\ttensor([5.4457], dtype=torch.float64)\n",
      " * 129\ttensor([1.8787], dtype=torch.float64)\n",
      " * 130\ttensor([-1.2931], dtype=torch.float64)\n",
      " * 131\ttensor([3.4707], dtype=torch.float64)\n",
      " * 132\ttensor([-0.7385], dtype=torch.float64)\n",
      " * 133\ttensor([1.5121], dtype=torch.float64)\n",
      " * 134\ttensor([0.6348], dtype=torch.float64)\n",
      " * 135\ttensor([-30.], dtype=torch.float64)\n",
      " * 136\ttensor([2.7729], dtype=torch.float64)\n",
      " * 137\ttensor([4.1683], dtype=torch.float64)\n",
      " * 138\ttensor([3.3241], dtype=torch.float64)\n",
      " * 139\ttensor([2.7909], dtype=torch.float64)\n",
      " * 140\ttensor([3.1825], dtype=torch.float64)\n",
      " * 141\ttensor([2.2539], dtype=torch.float64)\n",
      " * 142\ttensor([3.3326], dtype=torch.float64)\n",
      " * 143\ttensor([3.0869], dtype=torch.float64)\n",
      " * 144\ttensor([4.2207], dtype=torch.float64)\n",
      " * 145\ttensor([7.3022], dtype=torch.float64)\n",
      " * 146\ttensor([4.6941], dtype=torch.float64)\n",
      " * 147\ttensor([2.2999], dtype=torch.float64)\n",
      " * 148\ttensor([2.7331], dtype=torch.float64)\n",
      " * 149\ttensor([2.7844], dtype=torch.float64)\n",
      " * 150\ttensor([3.2781], dtype=torch.float64)\n",
      " * 151\ttensor([4.0091], dtype=torch.float64)\n",
      " * 152\ttensor([1.6445], dtype=torch.float64)\n",
      " * 153\ttensor([2.8153], dtype=torch.float64)\n",
      " * 154\ttensor([9.0914], dtype=torch.float64)\n",
      " * 155\ttensor([2.0723], dtype=torch.float64)\n",
      " * 156\ttensor([2.8803], dtype=torch.float64)\n",
      " * 157\ttensor([2.9596], dtype=torch.float64)\n",
      " * 158\ttensor([6.0605], dtype=torch.float64)\n",
      " * 159\ttensor([1.8927], dtype=torch.float64)\n",
      " * 160\ttensor([3.8258], dtype=torch.float64)\n",
      " * 161\ttensor([3.6301], dtype=torch.float64)\n",
      " * 162\ttensor([1.9799], dtype=torch.float64)\n",
      " * 163\ttensor([1.8137], dtype=torch.float64)\n",
      " * 164\ttensor([3.8840], dtype=torch.float64)\n",
      " * 165\ttensor([2.3868], dtype=torch.float64)\n",
      " * 166\ttensor([4.2373], dtype=torch.float64)\n",
      " * 167\ttensor([-30.], dtype=torch.float64)\n",
      " * 168\ttensor([7.1446], dtype=torch.float64)\n",
      " * 169\ttensor([2.7316], dtype=torch.float64)\n",
      " * 170\ttensor([-0.9113], dtype=torch.float64)\n",
      " * 171\ttensor([1.7116], dtype=torch.float64)\n",
      " * 172\ttensor([2.1011], dtype=torch.float64)\n",
      " * 173\ttensor([2.2517], dtype=torch.float64)\n",
      " * 174\ttensor([4.2555], dtype=torch.float64)\n",
      " * 175\ttensor([4.9676], dtype=torch.float64)\n",
      " * 176\ttensor([9.7064], dtype=torch.float64)\n",
      " * 177\ttensor([2.6783], dtype=torch.float64)\n",
      " * 178\ttensor([2.1691], dtype=torch.float64)\n",
      " * 179\ttensor([1.9952], dtype=torch.float64)\n",
      " * 180\ttensor([2.2691], dtype=torch.float64)\n",
      " * 181\ttensor([2.1759], dtype=torch.float64)\n",
      " * 182\ttensor([1.6560], dtype=torch.float64)\n",
      " * 183\ttensor([2.7769], dtype=torch.float64)\n",
      " * 184\ttensor([5.7557], dtype=torch.float64)\n",
      " * 185\ttensor([-30.], dtype=torch.float64)\n",
      " * 186\ttensor([2.8445], dtype=torch.float64)\n",
      " * 187\ttensor([1.7026], dtype=torch.float64)\n",
      " * 188\ttensor([1.0138], dtype=torch.float64)\n",
      " * 189\ttensor([-3.7285], dtype=torch.float64)\n",
      " * 190\ttensor([5.2328], dtype=torch.float64)\n",
      " * 191\ttensor([8.8731], dtype=torch.float64)\n",
      " * 192\ttensor([1.1345], dtype=torch.float64)\n",
      " * 193\ttensor([2.0083], dtype=torch.float64)\n",
      " * 194\ttensor([3.5954], dtype=torch.float64)\n",
      " * 195\ttensor([5.5324], dtype=torch.float64)\n",
      " * 196\ttensor([3.2819], dtype=torch.float64)\n",
      " * 197\ttensor([7.8537], dtype=torch.float64)\n",
      " * 198\ttensor([2.9514], dtype=torch.float64)\n",
      " * 199\ttensor([3.4134], dtype=torch.float64)\n",
      " * 200\ttensor([4.1391], dtype=torch.float64)\n",
      " * 201\ttensor([4.3725], dtype=torch.float64)\n",
      " * 202\ttensor([0.2613], dtype=torch.float64)\n",
      " * 203\ttensor([1.6721], dtype=torch.float64)\n",
      " * 204\ttensor([2.0420], dtype=torch.float64)\n",
      " * 205\ttensor([4.2336], dtype=torch.float64)\n",
      " * 206\ttensor([2.2465], dtype=torch.float64)\n",
      " * 207\ttensor([1.7041], dtype=torch.float64)\n",
      " * 208\ttensor([1.8495], dtype=torch.float64)\n",
      " * 209\ttensor([4.7426], dtype=torch.float64)\n",
      " * 210\ttensor([3.8824], dtype=torch.float64)\n",
      " * 211\ttensor([4.6033], dtype=torch.float64)\n",
      " * 212\ttensor([1.2113], dtype=torch.float64)\n",
      " * 213\ttensor([4.7843], dtype=torch.float64)\n",
      " * 214\ttensor([0.8372], dtype=torch.float64)\n",
      " * 215\ttensor([1.3669], dtype=torch.float64)\n",
      " * 216\ttensor([1.6305], dtype=torch.float64)\n",
      " * 217\ttensor([3.1663], dtype=torch.float64)\n",
      " * 218\ttensor([2.7164], dtype=torch.float64)\n",
      " * 219\ttensor([3.2823], dtype=torch.float64)\n",
      " * 220\ttensor([4.3965], dtype=torch.float64)\n",
      " * 221\ttensor([2.7384], dtype=torch.float64)\n",
      " * 222\ttensor([2.9244], dtype=torch.float64)\n",
      " * 223\ttensor([4.6554], dtype=torch.float64)\n",
      " * 224\ttensor([4.6207], dtype=torch.float64)\n",
      " * 225\ttensor([2.2558], dtype=torch.float64)\n",
      " * 226\ttensor([0.6911], dtype=torch.float64)\n",
      " * 227\ttensor([2.0357], dtype=torch.float64)\n",
      " * 228\ttensor([6.9845], dtype=torch.float64)\n",
      " * 229\ttensor([6.3266], dtype=torch.float64)\n",
      " * 230\ttensor([3.6775], dtype=torch.float64)\n",
      " * 231\ttensor([4.4482], dtype=torch.float64)\n",
      " * 232\ttensor([4.0405], dtype=torch.float64)\n",
      " * 233\ttensor([1.8221], dtype=torch.float64)\n",
      " * 234\ttensor([1.8365], dtype=torch.float64)\n",
      " * 235\ttensor([0.9399], dtype=torch.float64)\n",
      " * 236\ttensor([1.3532], dtype=torch.float64)\n",
      " * 237\ttensor([4.1358], dtype=torch.float64)\n",
      " * 238\ttensor([5.0809], dtype=torch.float64)\n",
      " * 239\ttensor([0.0253], dtype=torch.float64)\n",
      " * 240\ttensor([3.7497], dtype=torch.float64)\n",
      " * 241\ttensor([5.5089], dtype=torch.float64)\n",
      " * 242\ttensor([4.3996], dtype=torch.float64)\n",
      " * 243\ttensor([4.2599], dtype=torch.float64)\n",
      " * 244\ttensor([5.1527], dtype=torch.float64)\n",
      " * 245\ttensor([3.6877], dtype=torch.float64)\n",
      " * 246\ttensor([3.5163], dtype=torch.float64)\n",
      " * 247\ttensor([1.1467], dtype=torch.float64)\n",
      " * 248\ttensor([1.3866], dtype=torch.float64)\n",
      " * 249\ttensor([3.4515], dtype=torch.float64)\n",
      " * 250\ttensor([1.7210], dtype=torch.float64)\n",
      " * 251\ttensor([0.5321], dtype=torch.float64)\n",
      " * 252\ttensor([3.7429], dtype=torch.float64)\n",
      " * 253\ttensor([7.6230], dtype=torch.float64)\n",
      " * 254\ttensor([3.4499], dtype=torch.float64)\n",
      " * 255\ttensor([0.5852], dtype=torch.float64)\n",
      " * 256\ttensor([1.8778], dtype=torch.float64)\n",
      " * 257\ttensor([3.9280], dtype=torch.float64)\n",
      " * 258\ttensor([1.7423], dtype=torch.float64)\n",
      " * 259\ttensor([3.4077], dtype=torch.float64)\n",
      " * 260\ttensor([1.8585], dtype=torch.float64)\n",
      " * 261\ttensor([1.4966], dtype=torch.float64)\n",
      " * 262\ttensor([4.3714], dtype=torch.float64)\n",
      " * 263\ttensor([6.3566], dtype=torch.float64)\n",
      " * 264\ttensor([9.1863], dtype=torch.float64)\n",
      " * 265\ttensor([3.2112], dtype=torch.float64)\n",
      " * 266\ttensor([4.9697], dtype=torch.float64)\n",
      " * 267\ttensor([3.8664], dtype=torch.float64)\n",
      " * 268\ttensor([3.1478], dtype=torch.float64)\n",
      " * 269\ttensor([1.8692], dtype=torch.float64)\n",
      " * 270\ttensor([4.5478], dtype=torch.float64)\n",
      " * 271\ttensor([2.5989], dtype=torch.float64)\n",
      " * 272\ttensor([2.4289], dtype=torch.float64)\n",
      " * 273\ttensor([1.0575], dtype=torch.float64)\n",
      " * 274\ttensor([4.2751], dtype=torch.float64)\n",
      " * 275\ttensor([-0.1622], dtype=torch.float64)\n",
      " * 276\ttensor([1.7574], dtype=torch.float64)\n",
      " * 277\ttensor([7.0552], dtype=torch.float64)\n",
      " * 278\ttensor([0.2933], dtype=torch.float64)\n",
      " * 279\ttensor([5.9990], dtype=torch.float64)\n",
      " * 280\ttensor([4.9966], dtype=torch.float64)\n",
      " * 281\ttensor([5.4162], dtype=torch.float64)\n",
      " * 282\ttensor([5.2762], dtype=torch.float64)\n",
      " * 283\ttensor([3.1351], dtype=torch.float64)\n",
      " * 284\ttensor([2.7259], dtype=torch.float64)\n",
      " * 285\ttensor([2.7644], dtype=torch.float64)\n",
      " * 286\ttensor([2.9270], dtype=torch.float64)\n",
      " * 287\ttensor([5.6632], dtype=torch.float64)\n",
      " * 288\ttensor([3.7718], dtype=torch.float64)\n",
      " * 289\ttensor([3.0546], dtype=torch.float64)\n",
      " * 290\ttensor([3.6274], dtype=torch.float64)\n",
      " * 291\ttensor([2.3567], dtype=torch.float64)\n",
      " * 292\ttensor([9.7003], dtype=torch.float64)\n",
      " * 293\ttensor([2.4846], dtype=torch.float64)\n",
      " * 294\ttensor([6.3230], dtype=torch.float64)\n",
      " * 295\ttensor([1.4027], dtype=torch.float64)\n",
      " * 296\ttensor([4.4073], dtype=torch.float64)\n",
      " * 297\ttensor([3.4309], dtype=torch.float64)\n",
      " * 298\ttensor([1.1098], dtype=torch.float64)\n",
      " * 299\ttensor([5.9121], dtype=torch.float64)\n",
      " * 300\ttensor([4.9214], dtype=torch.float64)\n",
      " * 301\ttensor([3.8510], dtype=torch.float64)\n",
      " * 302\ttensor([0.3986], dtype=torch.float64)\n",
      " * 303\ttensor([4.5175], dtype=torch.float64)\n",
      " * 304\ttensor([2.6439], dtype=torch.float64)\n",
      " * 305\ttensor([5.8892], dtype=torch.float64)\n",
      " * 306\ttensor([3.7657], dtype=torch.float64)\n",
      " * 307\ttensor([6.1422], dtype=torch.float64)\n",
      " * 308\ttensor([2.1207], dtype=torch.float64)\n",
      " * 309\ttensor([0.9718], dtype=torch.float64)\n",
      " * 310\ttensor([2.0883], dtype=torch.float64)\n",
      " * 311\ttensor([4.2606], dtype=torch.float64)\n",
      " * 312\ttensor([2.8588], dtype=torch.float64)\n",
      " * 313\ttensor([3.7478], dtype=torch.float64)\n",
      " * 314\ttensor([5.0974], dtype=torch.float64)\n",
      " * 315\ttensor([4.3393], dtype=torch.float64)\n",
      " * 316\ttensor([4.2230], dtype=torch.float64)\n",
      " * 317\ttensor([3.4724], dtype=torch.float64)\n",
      " * 318\ttensor([4.9319], dtype=torch.float64)\n",
      " * 319\ttensor([1.2645], dtype=torch.float64)\n",
      " * 320\ttensor([2.1893], dtype=torch.float64)\n",
      " * 321\ttensor([3.1037], dtype=torch.float64)\n",
      " * 322\ttensor([1.9197], dtype=torch.float64)\n",
      " * 323\ttensor([4.4096], dtype=torch.float64)\n",
      " * 324\ttensor([5.8277], dtype=torch.float64)\n",
      " * 325\ttensor([4.5469], dtype=torch.float64)\n",
      " * 326\ttensor([6.0471], dtype=torch.float64)\n",
      " * 327\ttensor([1.3807], dtype=torch.float64)\n",
      " * 328\ttensor([5.0924], dtype=torch.float64)\n",
      " * 329\ttensor([2.9914], dtype=torch.float64)\n",
      " * 330\ttensor([1.0228], dtype=torch.float64)\n",
      " * 331\ttensor([4.4239], dtype=torch.float64)\n",
      " * 332\ttensor([2.6144], dtype=torch.float64)\n",
      " * 333\ttensor([0.5312], dtype=torch.float64)\n",
      " * 334\ttensor([3.8479], dtype=torch.float64)\n",
      " * 335\ttensor([2.0088], dtype=torch.float64)\n",
      " * 336\ttensor([3.3644], dtype=torch.float64)\n",
      " * 337\ttensor([0.1280], dtype=torch.float64)\n",
      " * 338\ttensor([1.6108], dtype=torch.float64)\n",
      " * 339\ttensor([4.0645], dtype=torch.float64)\n",
      " * 340\ttensor([3.6694], dtype=torch.float64)\n",
      " * 341\ttensor([2.1670], dtype=torch.float64)\n",
      " * 342\ttensor([2.5657], dtype=torch.float64)\n",
      " * 343\ttensor([4.3466], dtype=torch.float64)\n",
      " * 344\ttensor([6.1490], dtype=torch.float64)\n",
      " * 345\ttensor([4.0141], dtype=torch.float64)\n",
      " * 346\ttensor([2.8499], dtype=torch.float64)\n",
      " * 347\ttensor([3.4420], dtype=torch.float64)\n",
      " * 348\ttensor([3.1714], dtype=torch.float64)\n",
      " * 349\ttensor([6.6595], dtype=torch.float64)\n",
      " * 350\ttensor([1.1632], dtype=torch.float64)\n",
      " * 351\ttensor([5.9633], dtype=torch.float64)\n",
      " * 352\ttensor([1.9233], dtype=torch.float64)\n",
      " * 353\ttensor([2.7848], dtype=torch.float64)\n",
      " * 354\ttensor([3.6475], dtype=torch.float64)\n",
      " * 355\ttensor([1.3353], dtype=torch.float64)\n",
      " * 356\ttensor([3.7652], dtype=torch.float64)\n",
      " * 357\ttensor([6.6089], dtype=torch.float64)\n",
      " * 358\ttensor([2.7105], dtype=torch.float64)\n",
      " * 359\ttensor([3.3198], dtype=torch.float64)\n",
      " * 360\ttensor([3.4797], dtype=torch.float64)\n",
      " * 361\ttensor([4.2359], dtype=torch.float64)\n",
      " * 362\ttensor([3.9634], dtype=torch.float64)\n",
      " * 363\ttensor([3.6651], dtype=torch.float64)\n",
      " * 364\ttensor([4.8243], dtype=torch.float64)\n",
      " * 365\ttensor([1.9143], dtype=torch.float64)\n",
      " * 366\ttensor([4.8061], dtype=torch.float64)\n",
      " * 367\ttensor([4.1082], dtype=torch.float64)\n",
      " * 368\ttensor([1.3079], dtype=torch.float64)\n",
      " * 369\ttensor([2.5026], dtype=torch.float64)\n",
      " * 370\ttensor([3.2715], dtype=torch.float64)\n",
      " * 371\ttensor([6.7891], dtype=torch.float64)\n",
      " * 372\ttensor([1.9274], dtype=torch.float64)\n",
      " * 373\ttensor([2.1367], dtype=torch.float64)\n",
      " * 374\ttensor([3.0797], dtype=torch.float64)\n",
      " * 375\ttensor([5.2995], dtype=torch.float64)\n",
      " * 376\ttensor([3.7233], dtype=torch.float64)\n",
      " * 377\ttensor([1.6232], dtype=torch.float64)\n",
      " * 378\ttensor([2.1357], dtype=torch.float64)\n",
      " * 379\ttensor([4.2181], dtype=torch.float64)\n",
      " * 380\ttensor([1.7762], dtype=torch.float64)\n",
      " * 381\ttensor([1.0088], dtype=torch.float64)\n",
      " * 382\ttensor([9.6901], dtype=torch.float64)\n",
      " * 383\ttensor([5.4039], dtype=torch.float64)\n",
      " * 384\ttensor([0.5149], dtype=torch.float64)\n",
      " * 385\ttensor([-0.0839], dtype=torch.float64)\n",
      " * 386\ttensor([2.9351], dtype=torch.float64)\n",
      " * 387\ttensor([8.0229], dtype=torch.float64)\n",
      " * 388\ttensor([2.5881], dtype=torch.float64)\n",
      " * 389\ttensor([5.2788], dtype=torch.float64)\n",
      " * 390\ttensor([3.6343], dtype=torch.float64)\n",
      " * 391\ttensor([5.3873], dtype=torch.float64)\n",
      " * 392\ttensor([2.6817], dtype=torch.float64)\n",
      " * 393\ttensor([1.4941], dtype=torch.float64)\n",
      " * 394\ttensor([4.4038], dtype=torch.float64)\n",
      " * 395\ttensor([2.6380], dtype=torch.float64)\n",
      " * 396\ttensor([2.2635], dtype=torch.float64)\n",
      " * 397\ttensor([8.5606], dtype=torch.float64)\n",
      " * 398\ttensor([3.8866], dtype=torch.float64)\n",
      " * 399\ttensor([3.0920], dtype=torch.float64)\n",
      " * 400\ttensor([2.1820], dtype=torch.float64)\n",
      " * 401\ttensor([2.6625], dtype=torch.float64)\n",
      " * 402\ttensor([3.7229], dtype=torch.float64)\n",
      " * 403\ttensor([5.4416], dtype=torch.float64)\n",
      " * 404\ttensor([2.8787], dtype=torch.float64)\n",
      " * 405\ttensor([4.8299], dtype=torch.float64)\n",
      " * 406\ttensor([7.5794], dtype=torch.float64)\n",
      " * 407\ttensor([3.3137], dtype=torch.float64)\n",
      " * 408\ttensor([2.2104], dtype=torch.float64)\n",
      " * 409\ttensor([2.6650], dtype=torch.float64)\n",
      " * 410\ttensor([5.2299], dtype=torch.float64)\n",
      " * 411\ttensor([2.8985], dtype=torch.float64)\n",
      " * 412\ttensor([1.1043], dtype=torch.float64)\n",
      " * 413\ttensor([3.2005], dtype=torch.float64)\n",
      " * 414\ttensor([5.1818], dtype=torch.float64)\n",
      " * 415\ttensor([3.0401], dtype=torch.float64)\n",
      " * 416\ttensor([3.7471], dtype=torch.float64)\n",
      " * 417\ttensor([-0.0450], dtype=torch.float64)\n",
      " * 418\ttensor([4.8146], dtype=torch.float64)\n",
      " * 419\ttensor([3.1159], dtype=torch.float64)\n",
      " * 420\ttensor([4.5222], dtype=torch.float64)\n",
      " * 421\ttensor([3.2055], dtype=torch.float64)\n",
      " * 422\ttensor([1.6798], dtype=torch.float64)\n",
      " * 423\ttensor([3.9857], dtype=torch.float64)\n",
      " * 424\ttensor([0.6530], dtype=torch.float64)\n",
      " * 425\ttensor([4.4991], dtype=torch.float64)\n",
      " * 426\ttensor([3.3719], dtype=torch.float64)\n",
      " * 427\ttensor([4.2814], dtype=torch.float64)\n",
      " * 428\ttensor([4.1174], dtype=torch.float64)\n",
      " * 429\ttensor([4.0700], dtype=torch.float64)\n",
      " * 430\ttensor([4.4577], dtype=torch.float64)\n",
      " * 431\ttensor([4.2769], dtype=torch.float64)\n",
      " * 432\ttensor([4.1448], dtype=torch.float64)\n",
      " * 433\ttensor([2.6305], dtype=torch.float64)\n",
      " * 434\ttensor([5.2905], dtype=torch.float64)\n",
      " * 435\ttensor([4.7136], dtype=torch.float64)\n",
      " * 436\ttensor([3.1704], dtype=torch.float64)\n",
      " * 437\ttensor([0.6833], dtype=torch.float64)\n",
      " * 438\ttensor([3.5408], dtype=torch.float64)\n",
      " * 439\ttensor([2.0057], dtype=torch.float64)\n",
      " * 440\ttensor([3.7249], dtype=torch.float64)\n",
      " * 441\ttensor([1.7487], dtype=torch.float64)\n",
      " * 442\ttensor([3.4492], dtype=torch.float64)\n",
      " * 443\ttensor([3.9180], dtype=torch.float64)\n",
      " * 444\ttensor([2.3968], dtype=torch.float64)\n",
      " * 445\ttensor([5.7652], dtype=torch.float64)\n",
      " * 446\ttensor([1.2298], dtype=torch.float64)\n",
      " * 447\ttensor([0.4091], dtype=torch.float64)\n",
      " * 448\ttensor([5.1085], dtype=torch.float64)\n",
      " * 449\ttensor([5.6747], dtype=torch.float64)\n",
      " * 450\ttensor([0.6783], dtype=torch.float64)\n",
      " * 451\ttensor([0.7076], dtype=torch.float64)\n",
      " * 452\ttensor([-1.1809], dtype=torch.float64)\n",
      " * 453\ttensor([3.8316], dtype=torch.float64)\n",
      " * 454\ttensor([1.8143], dtype=torch.float64)\n",
      " * 455\ttensor([1.7386], dtype=torch.float64)\n",
      " * 456\ttensor([2.7139], dtype=torch.float64)\n",
      " * 457\ttensor([3.3056], dtype=torch.float64)\n",
      " * 458\ttensor([5.4401], dtype=torch.float64)\n",
      " * 459\ttensor([2.1087], dtype=torch.float64)\n",
      " * 460\ttensor([5.9845], dtype=torch.float64)\n",
      " * 461\ttensor([-0.2888], dtype=torch.float64)\n",
      " * 462\ttensor([7.5377], dtype=torch.float64)\n",
      " * 463\ttensor([3.7560], dtype=torch.float64)\n",
      " * 464\ttensor([2.6280], dtype=torch.float64)\n",
      " * 465\ttensor([5.2649], dtype=torch.float64)\n",
      " * 466\ttensor([1.3409], dtype=torch.float64)\n",
      " * 467\ttensor([1.9197], dtype=torch.float64)\n",
      " * 468\ttensor([3.5534], dtype=torch.float64)\n",
      " * 469\ttensor([2.3976], dtype=torch.float64)\n",
      " * 470\ttensor([2.6070], dtype=torch.float64)\n",
      " * 471\ttensor([2.3015], dtype=torch.float64)\n",
      " * 472\ttensor([1.6986], dtype=torch.float64)\n",
      " * 473\ttensor([4.1425], dtype=torch.float64)\n",
      " * 474\ttensor([3.3485], dtype=torch.float64)\n",
      " * 475\ttensor([3.7335], dtype=torch.float64)\n",
      " * 476\ttensor([1.0189], dtype=torch.float64)\n",
      " * 477\ttensor([-0.6005], dtype=torch.float64)\n",
      " * 478\ttensor([4.1203], dtype=torch.float64)\n",
      " * 479\ttensor([0.8036], dtype=torch.float64)\n",
      " * 480\ttensor([3.1100], dtype=torch.float64)\n",
      " * 481\ttensor([3.4630], dtype=torch.float64)\n",
      " * 482\ttensor([5.8314], dtype=torch.float64)\n",
      " * 483\ttensor([5.2997], dtype=torch.float64)\n",
      " * 484\ttensor([3.7156], dtype=torch.float64)\n",
      " * 485\ttensor([4.7121], dtype=torch.float64)\n",
      " * 486\ttensor([1.8866], dtype=torch.float64)\n",
      " * 487\ttensor([2.8332], dtype=torch.float64)\n",
      " * 488\ttensor([2.7270], dtype=torch.float64)\n",
      " * 489\ttensor([4.4262], dtype=torch.float64)\n",
      " * 490\ttensor([2.5799], dtype=torch.float64)\n",
      " * 491\ttensor([4.7985], dtype=torch.float64)\n",
      " * 492\ttensor([7.2403], dtype=torch.float64)\n",
      " * 493\ttensor([3.7662], dtype=torch.float64)\n",
      " * 494\ttensor([1.8671], dtype=torch.float64)\n",
      " * 495\ttensor([2.7882], dtype=torch.float64)\n",
      " * 496\ttensor([6.5622], dtype=torch.float64)\n",
      " * 497\ttensor([5.6184], dtype=torch.float64)\n",
      " * 498\ttensor([4.3831], dtype=torch.float64)\n",
      " * 499\ttensor([2.4190], dtype=torch.float64)\n",
      "plogp\t smiles\n",
      "13.511500358581543\tC=CC(CCCCCCC(=O)NCc1ccc([N+](=O)[O-])cc1)CCSCSCCCNC(=O)CCCCCCCCCNC(=O)C=CCCCCCCSCCCCCCCC\n",
      "9.706399917602539\tC=CCNC(=C(C)CNCCCCCCCCCCOCCOC(CCCCCCc1ccccc1)COC(=O)C(C)C)c1cccc(OC)c1\n",
      "9.700300216674805\tCC(O)C1CCCCCCCCCCCCN(C)CCC2CCC=CC(CC1O)CC1(C)CCCCC1CCC2\n",
      "9.690099716186523\tCC1CC2CCCCC3C(C)C(C)C3(C)CCC3C(C)(C)CC=CC3(C)CCCCC12\n",
      "9.325799942016602\tCC(N)CCCCCNC(=O)COCCCCCCCCCCCCNC(=O)NCCCCCCCCCNC(=Nc1ccccc1)c1ccccc1\n",
      "9.251999855041504\tC=CC1CCCCC(C)(NCCCCCCCCCC(C)C(C)OC=O)C(C)CC2CCC2CCC1\n",
      "9.186300277709961\tCOC1=NC2(C(CC(C)C)=NN(Cc3csc4ccccc34)CCN3C(=O)N(Cc4ccc(Cl)cc4Cl)CCC32)C(C)C(=O)C=Cc2ccccc21\n",
      "9.09142017364502\tCCOCc1ccccc1C=C(Cc1ccc(OC)cc1)NC(=O)NC(CC(c1cccc(C)c1)N1CCCCC1)C(=O)N1CCCCCN(C(=S)C(N)C=Cc2ccccc2)CC1\n",
      "8.873100280761719\tCC1=CC(C)C(O)OC=COC2OC(C)C(C(O)C(C)C(CC(C)C)C(OC(C)(C)C)C(C)(C)C)C(OCCC(C)C)CCCC12\n",
      "8.794500350952148\tCCOP1(=O)CN2CCC(c3ccccc3)(C=CC=CC=C3C=CC=CC=C4C=C5C(=O)C(=Cc6c(cnn64)NCCOCCCCN1)C=C(CO)N(C)C35c1cccc(Cl)c1)C2\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.acquisition import UpperConfidenceBound\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.utils.transforms import standardize, normalize, unnormalize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "\n",
    "from smiles_vocab import SmilesVocabulary\n",
    "from smiles_vae import SmilesVAE\n",
    "\n",
    "from rdkit import RDLogger\n",
    "\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "\n",
    "def bo_dataset_construction(\n",
    "    vae,\n",
    "    input_tensor,\n",
    "    smiles_list,\n",
    "    batch_size=128,\n",
    "    max_batch=10\n",
    "):\n",
    "    \"\"\"潜在空間上のデータセットへの変換を行う\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vae : SmilesVAE\n",
    "        SMILES-VAEのモデル\n",
    "    input_tensor : torch.Tensor\n",
    "        SMILES系列を整数系列として表現したテンソル\n",
    "    smiles_list : list[str]\n",
    "        SMILES系列のリスト\n",
    "    batch_size : int, optional\n",
    "        エンコーダを用いる際のバッチサイズ, by default 128\n",
    "    max_batch : int, optional\n",
    "        データセットとして用いるバッチ数, by default 10\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (torch.Tensor, torch.Tensor, list[str])\n",
    "        潜在ベクトルのテンソル、logPのテンソル、SMILES系列のリスト\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(\n",
    "        TensorDataset(input_tensor),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    z_list = []\n",
    "    plogp_list = []\n",
    "    out_smiles_list = []\n",
    "    for each_batch_idx, each_tensor in enumerate(dataloader):\n",
    "        if each_batch_idx == max_batch:\n",
    "            break\n",
    "        smiles_sublist = smiles_list[batch_size * each_batch_idx: batch_size * (each_batch_idx + 1)]\n",
    "        with torch.no_grad():\n",
    "            z, _ = vae.encode(each_tensor[0].to(vae.device))\n",
    "        z_list.append(z.to(\"cpu\").double())\n",
    "        plogp_tensor = compute_plogp(smiles_sublist)\n",
    "        plogp_list.append(plogp_tensor.double())\n",
    "        out_smiles_list.extend(smiles_sublist)\n",
    "    return torch.cat(z_list), torch.cat(plogp_list), out_smiles_list\n",
    "\n",
    "\n",
    "def obj_func(z, vae):\n",
    "    \"\"\"ベイズ最適化を行う対象の目的関数。潜在ベクトルを受け取ってそれに対応する分子に対する評価関数の値（logP）を返す。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : torch.Tensor\n",
    "        潜在ベクトル\n",
    "    vae : SmilesVAE\n",
    "        SMILES-VAEのモデル\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (torch.Tensor, list[str])\n",
    "        (評価関数の値、SMILESのリスト)\n",
    "    \"\"\"\n",
    "    z = z.to(torch.float32)\n",
    "    # SMILES-VAEは潜在ベクトルを正しいSMILES系列にデコードできる確率が高くないため正しいSMILES系列が得られるまでデコードを繰り返す\n",
    "    for _ in range(100):\n",
    "        smiles_list = vae.generate(z, deterministic=False)\n",
    "        success_list, failed_idx_list = filter_valid(smiles_list)\n",
    "        if success_list:\n",
    "            smiles_list = success_list[:1]\n",
    "            break\n",
    "    plogp_tensor = compute_plogp(smiles_list).double()\n",
    "    return plogp_tensor, smiles_list\n",
    "\n",
    "\n",
    "smiles_vocab = SmilesVocabulary()\n",
    "train_tensor, train_smiles_list = smiles_vocab.batch_update_from_file(\"data/train.smi\", with_smiles=True, ratio=0.1)\n",
    "val_tensor, val_smiles_list = smiles_vocab.batch_update_from_file(\"data/valid.smi\", with_smiles=True, ratio=0.1)\n",
    "max_len = train_tensor.shape[1]\n",
    "latent_dim = 64\n",
    "\n",
    "# SMILES-VAEの読み込み\n",
    "vae = SmilesVAE(\n",
    "    vocab=smiles_vocab, latent_dim=64, emb_dim=256,\n",
    "    encoder_params={\"hidden_size\": 512, \"num_layers\": 1, \"bidirectional\": False, \"dropout\": 0},\n",
    "    decoder_params={\"hidden_size\": 512, \"num_layers\": 1, \"dropout\": 0},\n",
    "    encoder2out_params={\"out_dim_list\": [256]},\n",
    "    max_len=max_len\n",
    ").to(\"cuda\")\n",
    "vae.load_state_dict(torch.load(\"data/vae.pt\"))\n",
    "vae.eval()\n",
    "\n",
    "# SMILES-VAEのエンコーダを用いて分子とその評価関数の値の対からなるデータセットを\n",
    "# 潜在空間上のデータセットDzに変換\n",
    "z_tensor, plogp_tensor, smiles_list = bo_dataset_construction(\n",
    "    vae=vae,\n",
    "    input_tensor=train_tensor,\n",
    "    smiles_list=train_smiles_list,\n",
    "    # batch_size=128,\n",
    ")\n",
    "n_trial = 500\n",
    "\n",
    "# 潜在空間上のデータセットDzをもとにベイズ最適化を行う\n",
    "for each_trial in range(n_trial):\n",
    "    # ガウス過程の入出力に対応するz_tensorおよびplotp_tensorを標準化する\n",
    "    standardized_y = standardize(plogp_tensor).reshape(-1, 1)\n",
    "    bounds = torch.stack([z_tensor.min(dim=0)[0],\n",
    "                          z_tensor.max(dim=0)[0]])\n",
    "    normalized_X = normalize(z_tensor, bounds)\n",
    "    # 上記のデータを用いてガウス過程を学習する\n",
    "    gp = SingleTaskGP(\n",
    "        train_X=normalized_X,\n",
    "        train_Y=standardized_y\n",
    "    )\n",
    "    # 獲得関数として信頼上限を計算し、獲得関数の最適化を通じて\n",
    "    # 次に目的関数の値を計算する候補点candidateを求める\n",
    "    mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "    fit_gpytorch_mll(mll)\n",
    "    UCB = UpperConfidenceBound(gp, beta=0.1)\n",
    "    candidate, acq_value = optimize_acqf(\n",
    "        acq_function=UCB,\n",
    "        bounds=torch.stack([-0.1 * torch.ones(latent_dim),\n",
    "                            1.1 * torch.ones(latent_dim)]),\n",
    "        q=1,\n",
    "        num_restarts=5,\n",
    "        raw_samples=10\n",
    "    )\n",
    "    # candidateは標準化された空間における点であるためこれをもとの入力空間に戻す\n",
    "    unnormalize_candidate = unnormalize(X=candidate, bounds=bounds)\n",
    "    # 目的関数の値を計算してデータセットを更新\n",
    "    plogp_val, each_smiles_list = obj_func(\n",
    "        z=unnormalize_candidate, vae=vae\n",
    "    )\n",
    "    z_tensor = torch.cat([z_tensor, unnormalize_candidate])\n",
    "    plogp_tensor = torch.cat([plogp_tensor, plogp_val])\n",
    "    smiles_list.extend(each_smiles_list)\n",
    "    print(f\" * {each_trial}\\t{plogp_val}\")\n",
    "\n",
    "plogp_tensor = plogp_tensor[-n_trial:]\n",
    "smiles_list = smiles_list[-n_trial:]\n",
    "_, ascending_idx_tensor = plogp_tensor.sort()\n",
    "\n",
    "# 見つかった分子のうち、logPの大きい分子上位10個を表示する\n",
    "print(\"plogp\\t smiles\")\n",
    "out_dict_list = []\n",
    "for each_idx in ascending_idx_tensor.tolist()[::-1][:10]:\n",
    "    print(f\"{plogp_tensor[each_idx]}\\t{smiles_list[each_idx]}\")\n",
    "    out_dict_list.append({\"smiles\": smiles_list[each_idx],\n",
    "                          \"plogp\": plogp_tensor[each_idx]})\n",
    "res_df = pd.DataFrame(out_dict_list)\n",
    "with gzip.open(\"data/smiles_vae_best_mol.pklz\", \"wb\") as f:\n",
    "    pickle.dump(res_df, f)\n",
    "\n",
    "with gzip.open(\"data/smiles_vae_bo_full.pklz\", \"wb\") as f:\n",
    "    pickle.dump((smiles_list, plogp_tensor), f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
