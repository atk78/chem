{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SELFIES.augm import Augmentation\n",
    "import selfies as sf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "aliphatic_organic = [\"B\", \"C\", \"N\", \"O\", \"S\", \"P\", \"F\", \"Cl\", \"Br\", \"I\"]\n",
    "aromatic_organic = [\"b\", \"c\", \"n\", \"o\", \"s\", \"p\"]\n",
    "bracket = [\"[\", \"]\"]  # includes isotope, symbol, chiral, hcount, charge, class\n",
    "bond = [\"-\", \"=\", \"#\", \"$\", \"/\", \"\\\\\", \".\"]\n",
    "lrb = [\"%\"]  # long ring bonds '%TWODIGITS'\n",
    "terminator = [\" \"]  # SPACE - start/end of selfies\n",
    "wildcard = [\"*\"]\n",
    "oov = [\"oov\"]  # out-of-vocabulary tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_tokens(smiles_array, split_l=1):\n",
    "    tokenized_smiles_list = list()\n",
    "    for ismiles in smiles_array.tolist():\n",
    "        tokenized_smiles_tmp = smiles_tokenizer(ismiles)\n",
    "        tokenized_smiles_list.append(\n",
    "            [\n",
    "                \"\".join(tokenized_smiles_tmp[i : i + split_l])\n",
    "                for i in range(0, len(tokenized_smiles_tmp) - split_l + 1, 1)\n",
    "            ]\n",
    "        )\n",
    "    return tokenized_smiles_list\n",
    "\n",
    "def smiles_to_selfies(smiles):\n",
    "    selfies = sf.encoder(smiles).split(sep=\"][\")\n",
    "    selfies[0] = selfies[0][1:]\n",
    "    selfies[-1] = selfies[-1][:-1]\n",
    "    selfies\n",
    "    return selfies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selfies_tokenizer(selfies):\n",
    "    selfies = selfies.split(sep=\"][\")\n",
    "    selfies[0] = selfies[0][1:]\n",
    "    selfies[-1] = selfies[-1][:-1]\n",
    "    selfies.insert(0, \" \")\n",
    "    selfies.extend(\" \")\n",
    "    selfies = [f\"[{s}]\" for s in selfies]\n",
    "    return selfies\n",
    "\n",
    "def get_tokens(selfies_array):\n",
    "    tokenized_selfies_list = list()\n",
    "    for i_selfies in selfies_array:\n",
    "        tokenized_selfies_tmp = selfies_tokenizer(i_selfies)\n",
    "        tokenized_selfies_list.append(tokenized_selfies_tmp)\n",
    "    return tokenized_selfies_list\n",
    "\n",
    "def extract_vocab(enum_tokens):\n",
    "    return set([itoken for i_selfies in enum_tokens for itoken in i_selfies])\n",
    "\n",
    "\n",
    "def add_extra_tokens(tokens, vocab_size):\n",
    "    tokens.insert(0, \"[unk]\")\n",
    "    tokens.insert(0, \"[pad]\")\n",
    "    vocab_size = vocab_size + 2\n",
    "    return tokens, vocab_size\n",
    "\n",
    "def get_tokentoint(tokens):\n",
    "    return dict((c, i) for i, c in enumerate(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[pad]': 0, '[unk]': 1, '[O-1]': 2, '[C@@H1]': 3, '[ ]': 4, '[\\\\Cl]': 5, '[\\\\O]': 6, '[\\\\C]': 7, '[/Cl]': 8, '[#C]': 9, '[=Branch1]': 10, '[=S]': 11, '[#Branch1]': 12, '[/C]': 13, '[I]': 14, '[#N]': 15, '[O]': 16, '[=O]': 17, '[S+2]': 18, '[P]': 19, '[=Branch2]': 20, '[=Ring1]': 21, '[Cl]': 22, '[Ring1]': 23, '[F]': 24, '[=Ring2]': 25, '[=N]': 26, '[Br]': 27, '[N]': 28, '[=N+1]': 29, '[#Branch2]': 30, '[S]': 31, '[/N]': 32, '[NH1]': 33, '[C@@]': 34, '[N+1]': 35, '[C]': 36, '[Ring2]': 37, '[Branch1]': 38, '[C@H1]': 39, '[Branch2]': 40, '[=P]': 41, '[=C]': 42, '[C@]': 43}\n",
      "[[ 0  0  0 ... 23 16  4]\n",
      " [ 0  0  0 ... 16 36  4]\n",
      " [ 0  0  0 ... 23 26  4]\n",
      " ...\n",
      " [ 0  0  0 ... 23 10  4]\n",
      " [ 0  0  0 ... 23 10  4]\n",
      " [ 0  0  0 ... 23 10  4]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filename = \"data/FreeSolv_SAMPL.csv\"\n",
    "prop = \"expt\"\n",
    "data = pd.read_csv(filename)\n",
    "data = data[[\"smiles\", prop]]\n",
    "smiles = data[\"smiles\"].values\n",
    "\n",
    "y = data[prop].values\n",
    "\n",
    "canonical = False\n",
    "rotation = True\n",
    "\n",
    "enum_selfies, enum_card, enum_prop = Augmentation(\n",
    "    smiles, y, canon=canonical, rotate=rotation\n",
    ")\n",
    "\n",
    "enum_tokens = get_tokens(selfies_array=enum_selfies)\n",
    "\n",
    "tokens = list(extract_vocab(enum_tokens))\n",
    "vocab_size = len(tokens)\n",
    "\n",
    "tokens, vocab_size = add_extra_tokens(tokens, vocab_size)\n",
    "max_length = np.max([len(i_selfies) for i_selfies in enum_tokens]) + 1\n",
    "vocab_int_dict = get_tokentoint(tokens)\n",
    "print(vocab_int_dict)\n",
    "int_selfies_array = np.zeros((len(enum_tokens), max_length), dtype=np.int32)\n",
    "\n",
    "for idx, i_selfies in enumerate(enum_tokens):\n",
    "    i_selfies_tmp = list()\n",
    "    if len(i_selfies) <= max_length:\n",
    "        i_selfies_tmp = [\"[pad]\"] * (max_length - len(i_selfies)) + i_selfies  # Force output vectors to have same length\n",
    "    else:\n",
    "        i_selfies_tmp = i_selfies[-max_length:]  # longer vectors are truncated (to be changed...)\n",
    "    integer_encoded = [vocab_int_dict[itoken] if (itoken in tokens) else vocab_int_dict[\"[unk]\"] for itoken in i_selfies_tmp]\n",
    "    int_selfies_array[idx] = integer_encoded\n",
    "print(int_selfies_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5355\n",
      "[' ', 'C', 'O', 'c', '1', 'c', 'c', 'c', '(', 'C', '(', '=', 'O', ')', 'N', '(', 'C', ')', 'C', ')', 'c', 'c', '1', ' ']\n",
      "集合にして語彙リストを作成\n",
      "{'5', ')', '[C@@H]', 'O', 'Br', 'c', '2', 'S', '[S+2]', 'N', '[C@H]', 'F', '-', '#', 'I', '(', 's', '4', 'P', '\\\\', 'Cl', '/', '[N+]', 'C', '=', '3', ' ', 'n', '[C@@]', '[nH]', '[C@]', '1', '[O-]'}\n",
      "tokensの数: 33\n",
      "リスト化\n",
      "['5', ')', '[C@@H]', 'O', 'Br', 'c', '2', 'S', '[S+2]', 'N', '[C@H]', 'F', '-', '#', 'I', '(', 's', '4', 'P', '\\\\', 'Cl', '/', '[N+]', 'C', '=', '3', ' ', 'n', '[C@@]', '[nH]', '[C@]', '1', '[O-]']\n",
      "33\n",
      "paddingとunkを追加\n",
      "['pad', 'unk', '5', ')', '[C@@H]', 'O', 'Br', 'c', '2', 'S', '[S+2]', 'N', '[C@H]', 'F', '-', '#', 'I', '(', 's', '4', 'P', '\\\\', 'Cl', '/', '[N+]', 'C', '=', '3', ' ', 'n', '[C@@]', '[nH]', '[C@]', '1', '[O-]']\n",
      "35\n",
      "トークン化されたSMILESの最大長\n",
      "51\n",
      "辞書化\n",
      "{'pad': 0, 'unk': 1, '5': 2, ')': 3, '[C@@H]': 4, 'O': 5, 'Br': 6, 'c': 7, '2': 8, 'S': 9, '[S+2]': 10, 'N': 11, '[C@H]': 12, 'F': 13, '-': 14, '#': 15, 'I': 16, '(': 17, 's': 18, '4': 19, 'P': 20, '\\\\': 21, 'Cl': 22, '/': 23, '[N+]': 24, 'C': 25, '=': 26, '3': 27, ' ': 28, 'n': 29, '[C@@]': 30, '[nH]': 31, '[C@]': 32, '1': 33, '[O-]': 34}\n",
      "[[ 0  0  0 ...  7 33 28]\n",
      " [ 0  0  0 ...  3 25 28]\n",
      " [ 0  0  0 ...  7 33 28]\n",
      " ...\n",
      " [ 0  0  0 ...  5 33 28]\n",
      " [ 0  0  0 ... 25 33 28]\n",
      " [ 0  0  0 ... 25 33 28]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "enum_tokens = token.get_tokens(smiles_array=enum)\n",
    "print(len(enum_tokens))\n",
    "print(enum_tokens[0])\n",
    "\n",
    "print(\"集合にして語彙リストを作成\")\n",
    "tokens = token.extract_vocab(enum_tokens)\n",
    "vocab_size = len(tokens)\n",
    "print(tokens)\n",
    "print(\"tokensの数:\", vocab_size)\n",
    "\n",
    "\n",
    "token.save_vocab(tokens, \"test.txt\")\n",
    "tokens = token.get_vocab(\"test.txt\")\n",
    "print(\"リスト化\")\n",
    "print(tokens)\n",
    "print(len(tokens))\n",
    "\n",
    "print(\"paddingとunkを追加\")\n",
    "tokens, vocab_size = token.add_extra_tokens(tokens, vocab_size)\n",
    "print(tokens)\n",
    "print(vocab_size)\n",
    "\n",
    "print(\"トークン化されたSMILESの最大長\")\n",
    "# +1は[unk]を含めるため\n",
    "max_length = np.max([len(ismiles) for ismiles in enum_tokens]) + 1\n",
    "print(max_length)\n",
    "\n",
    "# int_vec_encodeの中身\n",
    "print(\"辞書化\")\n",
    "vocab_int_dict = token.get_tokentoint(tokens)\n",
    "print(vocab_int_dict)\n",
    "\n",
    "int_smiles_array = np.zeros((len(enum_tokens), max_length), dtype=np.int32)\n",
    "\n",
    "for csmiles, ismiles in enumerate(enum_tokens):\n",
    "    ismiles_tmp = list()\n",
    "    if len(ismiles) <= max_length:\n",
    "        ismiles_tmp = [\"pad\"] * (\n",
    "            max_length - len(ismiles)\n",
    "        ) + ismiles  # Force output vectors to have same length\n",
    "    else:\n",
    "        ismiles_tmp = ismiles[\n",
    "            -max_length:\n",
    "        ]  # longer vectors are truncated (to be changed...)\n",
    "    integer_encoded = [\n",
    "        vocab_int_dict[itoken] if (itoken in tokens) else vocab_int_dict[\"unk\"]\n",
    "        for itoken in ismiles_tmp\n",
    "    ]\n",
    "    int_smiles_array[csmiles] = integer_encoded\n",
    "print(int_smiles_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5355\n"
     ]
    }
   ],
   "source": [
    "print(len(int_smiles_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
